{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch.vit_3d import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 128,          # image size\n",
    "    frames = 16,               # number of frames\n",
    "    image_patch_size = 16,     # image patch size\n",
    "    frame_patch_size = 2,      # frame patch size\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "video = torch.randn(4, 3, 16, 128, 128) # (batch, channels, frames, height, width)\n",
    "\n",
    "preds = v(video) # (4, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vit_pytorch import ViT, MAE\n",
    "from vit_pytorch.vit_3d import ViT\n",
    "\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 128,          # image size\n",
    "    frames = 128,               # number of frames\n",
    "    image_patch_size = 16,     # image patch size\n",
    "    frame_patch_size = 16,      # frame patch size\n",
    "    num_classes = 1000,\n",
    "    dim = 1024,\n",
    "    depth = 6,\n",
    "    heads = 8,\n",
    "    mlp_dim = 2048,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1\n",
    ")\n",
    "\n",
    "mae = MAE(\n",
    "    encoder = v,\n",
    "    masking_ratio = 0.75,   # the paper recommended 75% masked patches\n",
    "    decoder_dim = 128,      # paper showed good results with just 512\n",
    "    decoder_depth = 6       # anywhere from 1 to 8\n",
    ")\n",
    "\n",
    "images3d = torch.randn(4, 3, 128, 128, 128)\n",
    "\n",
    "loss = mae(images3d)\n",
    "print(loss)\n",
    "loss.backward()\n",
    "\n",
    "# that's all!\n",
    "# do the above in a for loop many times with a lot of images and your vision transformer will learn\n",
    "\n",
    "# save your improved vision transformer\n",
    "# torch.save(v.state_dict(), './trained-vit.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aicpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
